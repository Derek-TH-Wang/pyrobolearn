
from abc import ABCMeta, abstractmethod


class InitialStateGenerator(object):
    r"""Initial State Generator

    Initialize the state which will be given as the first state by the environment when calling ``env.reset()``.
    If for instance, the state consists of joint positions and velocities, we can generate them from a distribution
    that is given or learned from data.

    The `state generator` is tightly coupled with a `state` object.

    Sometimes a mapping between different states is necessary. For instance, the state generator might generate
    human joint states that need first to be mapped to robot joint states in order to initialize the robot.
    In this example, a kinematic mapping which is modeled mathematically or learned need to be provided additionally.
    This is particularly significant as robot data are lacking, while human data is pretty abundant.
    The mapping function has to return a `state` object.
    """
    __metaclass__ = ABCMeta

    def __init__(self):
        pass

    @abstractmethod
    def generate(self):
        pass


class FixedInitialStateGenerator(InitialStateGenerator):
    r"""Fixed Initial State Generator

    This generator returns the same initial state each time it is called.
    """
    def __init__(self):
        super(FixedInitialStateGenerator, self).__init__()


class FIFOQueueInitialStateGenerator(InitialStateGenerator):
    r"""FIFO Queue Initial State Generator

    Generate the initial state from a FIFO queue. If the queue is empty returns the default initial state.
    The queue is filled by the user during training.
    """
    def __init__(self):
        super(FIFOQueueInitialStateGenerator, self).__init__()


class PriorityQueueInitialStateGenerator(InitialStateGenerator):
    r"""Priority Queue Initial State Generator

    Generate the initial state from a priority queue filled by the user. If empty, it returns the default initial
    state. The queue can be filled for instance with states that have high/low uncertainty, or high/low rewards.

    The queue has a limited capacity, and can be used to include states from which the agent/policy performed
    poorly during the training.
    """
    def __init__(self):
        pass


class DistributedInitialStateGenerator():
    r"""Distributed Initial State Generator

    The initial states :math:`s` are generated by a probability distribution :math:`p(s)`, that is :math:`s \sim p(s)`.
    The probability distribution can be learned using generative models.
    """
    def __init__(self):
        pass


class UniformInitialStateGenerator():
    r"""Uniform Initial State Generator

    The initial states are generated by a uniform distribution. If no upper/lower limits are specified, the limits
    will be set to be the range of the states.
    """
    def __init__(self):
        pass


class NormalInitialStateGenerator():
    r"""Normal Initial State Generator

    The initial states are generated by a normal distribution, where the mean and standard deviation are specified.
    The states are then truncated / clipped to be inside their corresponding range.
    """
    def __init__(self):
        pass


class LearnableInitialStateGenerator():
    r"""Learnable Initial State Generator

    This uses Generative Models.
    """
    def __init__(self, model):
        self.model = model


class AEBOInitialStateGenerator():
    r"""AutoEncoder (AE) - Bayesian Optimization (BO) Initial State Generator

    Using a pretrained AE on plausible states, and keeping the decoder allows us to explore in the lower dimensional
    state space using BO (GP). The BO will provide the reduced state vector based on the uncertainty / objective
    fct value. Then, the outputted vector can be fed to the decoder which will return the corresponding high-
    dimensional state.

    In addition, we fix a certain capacity to the kernel matrix of the GP underlying the BO. If when inserting a
    new (low-dimensional) state, the capacity is exceeded, the oldest state is removed from the kernel to allow
    the incoming state.

    If the states have a certain range, we use the encoder part to get the corresponding low-dimensional state limits.
    The exploration will then be carried out in the hyperrectangle formed by these 2 reduced state vector limits.
    """
    def __init__(self, autoencoder, kernel_capacity=100):
        pass


class VAEInitialStateGenerator():
    r"""Variational Autoencoder (VAE) Initial State Generator

    This uses the decoder a pretrained VAE to generate initial states.
    """
    def __init__(self, states, model):
        pass


class GANInitialStateGenerator():
    r"""Generative Adversarial Network (GAN) Initial State Generator

    This uses the generator of a trained GAN model to generate similar states.
    """

    def __init__(self, states, model, distribution=None, mapping=None):
        """

        :param states: states that need to be generated
        :param model: GAN or generator of GAN
        :param distribution: distribution over the noise vector
        :param mapping:
        """

        # checking and setting the model
        if isinstance(model, GAN):
            self.generator = model.getGenerator()
        elif isinstance(model, Generator):
            self.generator = model
        else:
            raise TypeError("The `model` parameter should be an instance of GAN or Generator.")

        # checking and setting the distribution
        if distribution is None:
            # create normal distribution with dimension of the generator input
            pass
        else:
            if not isinstance(distribution, Distribution):
                raise TypeError("The given `distribution` is not an instance of Distribution.")

        self.distribution = distribution

        # setting mapping
        self.mapping = mapping

    def generate(self):
        noise_vector = self.distribution.sample()
        states = self.generator(noise_vector)
        if self.mapping is not None:
            return self.mapping(states)
        return states


class GMMInitialStateGenerator():
    r"""Gaussian Mixture Model Initial State Generator

    This uses a pretrained GMM to generate the states.
    """

    def __init__(self):
        pass
